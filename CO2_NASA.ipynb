{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2015.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_1.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2016.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_2.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2017.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_3.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2018.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_4.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2019.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_5.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2020.jpg processed and saved to C:\\Users\\Hinal\\OneDrive\\Mtech Sem 1\\LAB WORK\\PDS\\OpenCV\\image_data_6.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Define paths for your images\n",
    "image_paths = [\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2015.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2016.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2017.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2018.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2019.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2020.jpg\"\n",
    "]\n",
    "\n",
    "# Step 2: Function to process and save each image\n",
    "def process_image(image_path, output_csv):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract pixel data\n",
    "    rows, cols = gray_image.shape\n",
    "    data = []\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            pixel_value = gray_image[i, j]\n",
    "            data.append([i, j, pixel_value])  # Coordinates (i, j) and intensity\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Row', 'Col', 'Intensity'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Image {image_path} processed and saved to {output_csv}\")\n",
    "\n",
    "# Step 3: Process each image\n",
    "output_dir = 'C:\\\\Users\\\\Hinal\\\\OneDrive\\\\Mtech Sem 1\\\\LAB WORK\\\\PDS\\\\OpenCV'\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    output_csv = os.path.join(output_dir, f'image_data_{idx + 1}.csv')\n",
    "    process_image(image_path, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed and combined into one CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Process each image and combine into a single DataFrame\n",
    "combined_data = []\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    rows, cols = gray_image.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            pixel_value = gray_image[i, j]\n",
    "            combined_data.append([idx + 1, i, j, pixel_value])  # Image index, coordinates, intensity\n",
    "\n",
    "# Convert combined data to DataFrame\n",
    "df_combined = pd.DataFrame(combined_data, columns=['Image', 'Row', 'Col', 'Intensity'])\n",
    "\n",
    "# Save to a single CSV\n",
    "df_combined.to_csv('C:\\\\Users\\\\Hinal\\\\OneDrive\\\\Mtech Sem 1\\\\LAB WORK\\\\PDS\\\\OpenCV\\\\combined_image_data.csv', index=False)\n",
    "print(\"All images processed and combined into one CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\hinal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-3.1.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.0/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.1.0-cp312-cp312-win_amd64.whl (294 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.1.0 sqlalchemy-2.0.35\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Step 1: Define paths for your images\n",
    "image_paths = [\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2015.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2016.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2017.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2018.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2019.jpg\",\n",
    "    \"images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2020.jpg\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_csv, year):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale (skip if already black and white)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Image dimensions and geo-coordinates mapping\n",
    "    rows, cols = gray_image.shape\n",
    "    lat_start, lat_end = 90, -90  # Latitude range from 90째N to 90째S\n",
    "    lon_start, lon_end = -180, 180  # Longitude range from -180째W to 180째E\n",
    "\n",
    "    # Calculate the latitude and longitude step per pixel\n",
    "    lat_step = (lat_start - lat_end) / rows\n",
    "    lon_step = (lon_end - lon_start) / cols\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    data = []\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            lat = lat_start - i * lat_step\n",
    "            lon = lon_start + j * lon_step\n",
    "            pixel_value = gray_image[i, j]\n",
    "            data.append([year, lat, lon, pixel_value])\n",
    "\n",
    "    # Convert to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data, columns=['Year', 'Latitude', 'Longitude', 'Intensity'])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Image {image_path} processed and saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2015.jpg processed and saved to output_csvs\\CO2_budget_2015.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2016.jpg processed and saved to output_csvs\\CO2_budget_2016.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2017.jpg processed and saved to output_csvs\\CO2_budget_2017.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2018.jpg processed and saved to output_csvs\\CO2_budget_2018.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2019.jpg processed and saved to output_csvs\\CO2_budget_2019.csv\n",
      "Image images/pilot_topdown_IS_NCE_CO2_Budget_grid_v1_2020.jpg processed and saved to output_csvs\\CO2_budget_2020.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'output_csvs'  # Output directory for the CSV files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    year = 2015 + idx  # Image year corresponds to 2015 through 2020\n",
    "    output_csv = os.path.join(output_dir, f'CO2_budget_{year}.csv')\n",
    "    process_image(image_path, output_csv, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed and combined into output_csvs\\combined_CO2_budget_data_2015_2020.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine all processed CSVs into one DataFrame and save\n",
    "combined_data = []\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    year = 2015 + idx\n",
    "    output_csv = os.path.join(output_dir, f'CO2_budget_{year}.csv')\n",
    "    df = pd.read_csv(output_csv)\n",
    "    combined_data.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "df_combined = pd.concat(combined_data)\n",
    "\n",
    "# Save combined data into a single CSV file\n",
    "combined_csv_path = os.path.join(output_dir, 'combined_CO2_budget_data_2015_2020.csv')\n",
    "df_combined.to_csv(combined_csv_path, index=False)\n",
    "print(f\"All images processed and combined into {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 58\u001b[0m\n\u001b[0;32m     49\u001b[0m db_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;66;03m# Your host\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5432\u001b[39m\u001b[38;5;124m'\u001b[39m,         \u001b[38;5;66;03m# Default PostgreSQL port\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNASA_U2\u001b[39m\u001b[38;5;124m'\u001b[39m     \u001b[38;5;66;03m# Replace with your PostgreSQL database name\u001b[39;00m\n\u001b[0;32m     55\u001b[0m }\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Step 6: Save combined data to PostgreSQL\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43msave_to_postgres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCO2_budget_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 35\u001b[0m, in \u001b[0;36msave_to_postgres\u001b[1;34m(df, table_name, db_params)\u001b[0m\n\u001b[0;32m     32\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(conn_string)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Create database if it does not exist\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mcreate_database_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Create table schema if it does not exist\u001b[39;00m\n\u001b[0;32m     38\u001b[0m create_table_if_not_exists(engine, table_name)\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mcreate_database_if_not_exists\u001b[1;34m(db_params)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a connection to PostgreSQL\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Execute SQL to create database if it doesn't exist\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     conn\u001b[38;5;241m.\u001b[39mexecute(\u001b[43mtext\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDO $$ BEGIN IF NOT EXISTS (SELECT 1 FROM pg_database WHERE datname = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) THEN CREATE DATABASE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; END IF; END $$;\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "def create_database_if_not_exists(db_params):\n",
    "    # Create connection string for PostgreSQL\n",
    "    conn_string = f\"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/postgres\"\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(conn_string)\n",
    "    \n",
    "    # Create a connection to PostgreSQL\n",
    "    with engine.connect() as conn:\n",
    "        # Execute SQL to create database if it doesn't exist\n",
    "        conn.execute(text(f\"DO $$ BEGIN IF NOT EXISTS (SELECT 1 FROM pg_database WHERE datname = '{db_params['dbname']}') THEN CREATE DATABASE {db_params['dbname']}; END IF; END $$;\"))\n",
    "        print(f\"Database {db_params['dbname']} is ready.\")\n",
    "\n",
    "def create_table_if_not_exists(engine, table_name):\n",
    "    # Create table schema manually if needed\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            \"Image\" INTEGER,\n",
    "            \"Row\" INTEGER,\n",
    "            \"Col\" INTEGER,\n",
    "            \"Intensity\" INTEGER\n",
    "        )\n",
    "        \"\"\"))\n",
    "        print(f\"Table {table_name} is ready.\")\n",
    "\n",
    "def save_to_postgres(df, table_name, db_params):\n",
    "    # Create connection string for PostgreSQL\n",
    "    conn_string = f\"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(conn_string)\n",
    "\n",
    "    # Create database if it does not exist\n",
    "    create_database_if_not_exists(db_params)\n",
    "    \n",
    "    # Create table schema if it does not exist\n",
    "    create_table_if_not_exists(engine, table_name)\n",
    "    \n",
    "    # Log the DataFrame columns and first few rows\n",
    "    print(\"DataFrame columns:\", df.columns)\n",
    "    print(\"First few rows:\\n\", df.head())\n",
    "    \n",
    "    # Save DataFrame to PostgreSQL\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    print(f\"Data saved to PostgreSQL table {table_name}\")\n",
    "\n",
    "# Parameters for the PostgreSQL connection\n",
    "db_params = {\n",
    "    'host': 'localhost',    # Your host\n",
    "    'port': '5432',         # Default PostgreSQL port\n",
    "    'user': 'postgres',     # Replace with your PostgreSQL username\n",
    "    'password': '26022002', # Replace with your PostgreSQL password\n",
    "    'dbname': 'NASA_U2'     # Replace with your PostgreSQL database name\n",
    "}\n",
    "\n",
    "# Step 6: Save combined data to PostgreSQL\n",
    "save_to_postgres(df_combined, 'CO2_budget_data', db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
